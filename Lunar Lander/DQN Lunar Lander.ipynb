{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import collections;\n",
    "import random;\n",
    "import gym;\n",
    "from copy import deepcopy;\n",
    "import tensorflow as tf;\n",
    "import matplotlib.pyplot as plt;\n",
    "import time;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BriefHistory:\n",
    "    def __init__(self,history_size):\n",
    "        self.history_size=history_size\n",
    "        self.current_state_queue=collections.deque(maxlen=history_size)\n",
    "        self.action_queue=collections.deque(maxlen=history_size)\n",
    "        self.next_state_queue=collections.deque(maxlen=history_size)\n",
    "        self.reward_queue=collections.deque(maxlen=history_size)\n",
    "        self.done_queue=collections.deque(maxlen=history_size)\n",
    "        self.queues=[self.current_state_queue,self.action_queue,\\\n",
    "                     self.next_state_queue,self.reward_queue,\\\n",
    "                     self.done_queue]\n",
    "        self.count=0\n",
    "        \n",
    "    def append(self,current_state,action,next_state,reward,done):\n",
    "        self.current_state_queue.append(current_state)\n",
    "        self.action_queue.append(action)\n",
    "        self.next_state_queue.append(next_state)\n",
    "        self.reward_queue.append(reward)\n",
    "        self.done_queue.append(done)\n",
    "        self.count+=1\n",
    "        \n",
    "    @property\n",
    "    def current_state_stacked(self):\n",
    "        return np.array(self.current_state_queue)\n",
    "    \n",
    "    @property\n",
    "    def action_stacked(self):\n",
    "        return np.array(self.action_queue)\n",
    "    \n",
    "    @property\n",
    "    def next_state_stacked(self):\n",
    "        return np.array(self.next_state_queue)\n",
    "    \n",
    "    @property\n",
    "    def reward_stacked(self):\n",
    "        return np.array(self.reward_queue)\n",
    "    \n",
    "    @property\n",
    "    def done_stacked(self):\n",
    "        return np.array(self.done_queue)\n",
    "    \n",
    "    @property\n",
    "    def full(self):\n",
    "        return self.count>=self.history_size  \n",
    "    \n",
    "    def __iter__(self):\n",
    "        for current_state,action,next_state,reward,done in zip(*self.queues):\n",
    "            yield current_state,action,next_state,reward,done\n",
    "    \n",
    "    \n",
    "    \n",
    "class Agent:\n",
    "\n",
    "    def __init__(self,env=gym.make(\"LunarLander-v2\"),q_online=None,\\\n",
    "                 history_size=20000,number_of_episodes=1000,epsilon=1,\\\n",
    "                 gamma=0.99,optimizer_lr=0.0001,seed=None,epsilon_coef=500,tau=0.005):\n",
    "        self.tau=tau\n",
    "        self.seed=seed\n",
    "        np.random.seed(self.seed)\n",
    "        random.seed(self.seed)\n",
    "        tf.random.set_random_seed(self.seed)\n",
    "        tf.compat.v1.set_random_seed(self.seed)\n",
    "        self.env=env\n",
    "        self.env.seed(self.seed)\n",
    "        self.epsilon=epsilon\n",
    "        self.gamma=gamma\n",
    "        self.optimizer_lr=optimizer_lr\n",
    "        if q_online==None:\n",
    "            self.q_online=self.initialize_q()\n",
    "            self.q_offline=self.initialize_q()\n",
    "        else:\n",
    "            self.q_online=q_online\n",
    "        \n",
    "        self.history=BriefHistory(history_size)\n",
    "        self.target_update_interval=10\n",
    "        self.number_of_episodes=number_of_episodes\n",
    "        self.total_rewards=[]\n",
    "        self.epsilon_coef=epsilon_coef\n",
    "    def initialize_q(self):\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=(self.env.observation_space.shape[0],))\n",
    "        initializer=tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=self.seed)\n",
    "        w = tf.keras.layers.Dense(128, activation='relu',kernel_initializer=initializer)(inputs)\n",
    "        w = tf.keras.layers.Dense(64, activation='relu',kernel_initializer=initializer)(w)\n",
    "        w = tf.keras.layers.Dense(32, activation='relu',kernel_initializer=initializer)(w)\n",
    "        output = tf.keras.layers.Dense(self.env.action_space.n,kernel_initializer=initializer)(w)\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "        optimizer = tf.keras.optimizers.RMSprop(lr=self.optimizer_lr)\n",
    "        model.compile(loss='mse', optimizer=optimizer);\n",
    "        return model\n",
    "    \n",
    "    def get_action(self,current_state):\n",
    "        is_random=random.random()<self.epsilon\n",
    "        if is_random:\n",
    "            action=np.random.randint(low=0,high=4)\n",
    "        else:\n",
    "            action=self.get_best_action_q_online(current_state)\n",
    "        return action\n",
    "\n",
    "    def run(self):\n",
    "        for i in range(self.number_of_episodes):\n",
    "            total_reward=0\n",
    "            done=False\n",
    "            current_state=self.env.reset()\n",
    "            count=0\n",
    "            while not done:\n",
    "                count+=1\n",
    "                action=self.get_action(current_state)\n",
    "                next_state,reward,done,_=self.env.step(action)\n",
    "                self.history.append(current_state,action,next_state,reward,done)\n",
    "                total_reward+=reward\n",
    "                current_state=next_state\n",
    "                self.experience_replay()\n",
    "                self.update_q_offline()\n",
    "            if (i+1)%25==0 or i==0:\n",
    "                print(\"Episode: {:<4}     Episode Reward: {:<4}     Epsilon: {:.2f}     Length of Episode: {:<4}\".format(i+1,int(total_reward),self.epsilon,count))\n",
    "            self.decay_epsilon()\n",
    "            self.total_rewards.append(total_reward)\n",
    "            \n",
    "\n",
    "\n",
    "    def experience_replay(self,verbose=0):\n",
    "        x=self.history.current_state_stacked\n",
    "        sampled_index=np.random.randint(low=0,high=len(x),size=100)\n",
    "        y=self.get_ys(sampled_index)\n",
    "        self.q_online.fit(x[sampled_index],y,epochs=1,shuffle=False,batch_size=20,verbose=verbose)      \n",
    "\n",
    "    def update_q_offline(self):\n",
    "        updated_weights=np.array(self.q_offline.get_weights())*(1-self.tau)+self.tau*np.array(self.q_online.get_weights())\n",
    "        self.q_offline.set_weights(updated_weights)        \n",
    "\n",
    "    \n",
    "    def get_ys(self,sample_index):\n",
    "        n=len(sample_index)\n",
    "        ys=self.q_online.predict(self.history.current_state_stacked[sample_index])\n",
    "        ys[range(n),self.history.action_stacked[sample_index]]=self.history.reward_stacked[sample_index]\n",
    "        not_done_indices=~self.history.done_stacked[sample_index]\n",
    "        q_star=self.q_offline.predict(self.history.next_state_stacked[sample_index]).max(axis=1)\n",
    "        ys[range(n),self.history.action_stacked[sample_index]]+=self.gamma*q_star*not_done_indices\n",
    "        return ys\n",
    "    \n",
    "    \n",
    "    def get_best_action_q_online(self,state):\n",
    "        xs=np.array([state])\n",
    "        ys=self.q_online.predict(xs)\n",
    "        return np.argmax(ys)\n",
    "        \n",
    "    def get_q_offline(self,state):\n",
    "        return self.q_offline.predict(np.array(state).reshape(1,-1))[0]\n",
    "    \n",
    "    def get_q_online(self,state):\n",
    "        return self.q_online.predict(np.array(state).reshape(1,-1))[0]\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon=10**(-len(self.total_rewards)/self.epsilon_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent()\n",
    "agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"LunarLander-v2\")\n",
    "done=False\n",
    "state=env.reset()\n",
    "env.render()\n",
    "while not done:\n",
    "    action=agent.get_best_action_q_online(state)\n",
    "    state,_,done,_=env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
